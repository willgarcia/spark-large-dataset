{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "# Project Title\n",
    "### Data Engineering Capstone Project\n",
    "\n",
    "#### Project Summary\n",
    "\n",
    "This project exercises an ETL pipeline on Spark based on large datasets (immigration and weather data). \n",
    "\n",
    "What can the data be used for?\n",
    "\n",
    "The database that will be created from the initial datasets will be used to analyse correlations between immigration data (ie. gener), temperature, in major cities.\n",
    "\n",
    "Example of questions the data model should be able to answer:\n",
    "\n",
    "* number of visitors per month, per year in a given city, state or country\n",
    "* seasonality of visits, depending on weather temperatures over the course of a week, month, year\n",
    "* main reasons of visits based on the VISA types ([B1, B2, WT, WB, others](https://isso.ucsf.edu/what-b-1b-2-and-wbwt-status)\n",
    "* impact of weather on tourism / regional economy by looking at the most/least popular destinations (city, state or country)\n",
    "\n",
    "Who can use it? \n",
    "\n",
    "The produced data can be used by a variety of end users such as:\n",
    "* Government workers: the data could help the immigration department to anticipate and re-evaluate future workloads/resource capacity/quotas in processing VISA requests\n",
    "* Tourism & Affairs workers: the data could help public or private organisations to understand the influence of climate or to understand the success of developments effort in the tourism industry.\n",
    "* Economists, historians: given the weather dataset goes back to ~1700, studying migrations trends could be another angle of analysis for historians or economists looking at the impact of global changes and their mass population effects.\n",
    "\n",
    "The project follows the follow steps:\n",
    "* Step 1: Scope the Project and Gather Data\n",
    "* Step 2: Explore and Assess the Data\n",
    "* Step 3: Define the Data Model\n",
    "* Step 4: Run ETL to Model the Data\n",
    "* Step 5: Complete Project Write Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Do all imports and installs here\n",
    "import pandas as pd\n",
    "import datetime\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import isnan, when, count, col, udf, col, udf, date_format, hour, year, month, weekofyear, dayofmonth, from_unixtime\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 1: Scope the Project and Gather Data\n",
    "\n",
    "#### Scope \n",
    "Explain what you plan to do in the project in more detail. What data do you use? What is your end solution look like? What tools did you use? etc>\n",
    "\n",
    "#### Describe and Gather Data \n",
    "Describe the data sets you're using. Where did it come from? What type of information is included? \n",
    "\n",
    "##### I94 Immigration Data\n",
    "\n",
    "[This](https://travel.trade.gov/research/reports/i94/historical/2016.html) data comes from the US National Tourism and Trade Office. A data dictionary is included in the workspace. This is where the data comes from. There's a sample file so you can take a look at the data in csv format before reading it all in. You do not have to use the entire dataset, just use what you need to accomplish the goal you set at the beginning of the project.\n",
    "\n",
    "Format:\n",
    "\n",
    "```\n",
    ",cicid,i94yr,i94mon,i94cit,i94res,i94port,arrdate,i94mode,i94addr,depdate,i94bir,i94visa,count,dtadfile,visapost,occup,entdepa,entdepd,entdepu,matflag,biryear,dtaddto,gender,insnum,airline,admnum,fltno,visatype\n",
    "2027561,4084316.0,2016.0,4.0,209.0,209.0,HHW,20566.0,1.0,HI,20573.0,61.0,2.0,1.0,20160422,,,G,O,,M,1955.0,07202016,F,,JL,56582674633.0,00782,WT\n",
    "```\n",
    "\n",
    "##### World Temperature Data\n",
    "\n",
    "This dataset came from Kaggle. You can read more about it [here](https://www.kaggle.com/berkeleyearth/climate-change-earth-surface-temperature-data).\n",
    "\n",
    "##### U.S. City Demographic Data\n",
    "\n",
    "This data comes from OpenSoft. You can read more about it [here](https://public.opendatasoft.com/explore/dataset/us-cities-demographics/export/).\n",
    "\n",
    "Format:\n",
    "\n",
    "```\n",
    "City;State;Median Age;Male Population;Female Population;Total Population;Number of Veterans;Foreign-born;Average Household Size;State Code;Race;Count\n",
    "Newark;New Jersey;34.6;138040;143873;281913;5829;86253;2.73;NJ;White;76402\n",
    "```\n",
    "\n",
    "##### Airport Code Table\n",
    "\n",
    "This is a simple table of airport codes and corresponding cities. It comes from [here](https://datahub.io/core/airport-codes#data).\n",
    "\n",
    "Format: \n",
    "\n",
    "```\n",
    "ident,type,name,elevation_ft,continent,iso_country,iso_region,municipality,gps_code,iata_code,local_code,coordinates\n",
    "00A,heliport,Total Rf Heliport,11,NA,US,US-PA,Bensalem,00A,,00A,\"-74.93360137939453, 40.07080078125\"\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "spark = SparkSession\\\n",
    "    .builder\\\n",
    "    .config(\"spark.jars.packages\",\"saurfang:spark-sas7bdat:2.0.0-s_2.11\")\\\n",
    "    .enableHiveSupport()\\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "travel_df = spark\\\n",
    "        .read\\\n",
    "        .format(\"com.github.saurfang.sas.spark\")\\\n",
    "        .load('../../data/18-83510-I94-Data-2016/i94_apr16_sub.sas7bdat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "temp_df = spark\\\n",
    "        .read\\\n",
    "        .option(\"header\", \"true\")\\\n",
    "        .csv('../../data2/GlobalLandTemperaturesByCity.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "demog_df = spark\\\n",
    "        .read\\\n",
    "        .option(\"header\", \"true\")\\\n",
    "        .option(\"delimiter\", \";\")\\\n",
    "        .csv('./us-cities-demographics.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Downsize datasets to 10k records during development\n",
    "# This is also to avoid filling up the Udacity workspace disk space as parquet files are persisted and used during the tests.\n",
    "demog_df = demog_df.limit(10000)\n",
    "travel_df = travel_df.limit(10000)\n",
    "temp_df = temp_df.limit(10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 2: Explore and Assess the Data\n",
    "#### Explore the Data \n",
    "Identify data quality issues, like missing values, duplicate data, etc.\n",
    "\n",
    "#### Cleaning Steps\n",
    "Document steps necessary to clean the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "def debug_df(spark_df): \n",
    "    \"\"\"\n",
    "    Show statistics on a dataframe.\n",
    "    Do not run this function in production as it runs full scans on the dataset\n",
    "    \"\"\"\n",
    "    print(\"Dataframe row count: {}\".format(spark_df.count()))\n",
    "    print(\"Dataframe first 2 rows: {}\".format(spark_df.show(n=2)))\n",
    "    print(\"Dataframe schema: {}\".format(spark_df.printSchema()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "##### Travel / Immigration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "def get_valid_codes(file, isnumeric):\n",
    "    \"\"\"\n",
    "    Takes a file (1 col) and return the values in a list\n",
    "    \"\"\"\n",
    "    valid_codes = {}\n",
    "    with open(file) as codes:\n",
    "        count = 0\n",
    "        for code in codes:\n",
    "            count = count + 1\n",
    "            col1 = code.split(';')[0].rstrip(\"\\n\")\n",
    "            if isnumeric:\n",
    "                valid_codes[count]=float(col1)\n",
    "            else:\n",
    "                valid_codes[count]=col1\n",
    "    return list(valid_codes.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "@udf\n",
    "def sasdate_to_datetime(sas_date):\n",
    "    \"\"\"\n",
    "    Convert SAS Date to Spark Date\n",
    "    \"\"\"\n",
    "    epoch = datetime.datetime(1960, 1, 1).date()\n",
    "    date = epoch + datetime.timedelta(sas_date)\n",
    "    return date.isoformat()\n",
    "\n",
    "@udf\n",
    "def sanitize(label):   \n",
    "    \"\"\"\n",
    "    Utility function to normalise locations (countries or cities)\n",
    "    \"\"\"\n",
    "    return label.upper().rstrip(\"\\n\").strip()\n",
    "\n",
    "cities = {}\n",
    "with open(\"./i94codes/I94PORT.codes\") as codes:\n",
    "    count = 0\n",
    "    for code in codes:\n",
    "        data = code.split(';')\n",
    "        col1 = data[0].rstrip()\n",
    "        col2 = data[1].rstrip(\"\\n\").strip()\n",
    "        cities[col1] = col2\n",
    "\n",
    "@udf\n",
    "def citycode_to_name(city_code):\n",
    "    \"\"\"\n",
    "    Return city name from code\n",
    "    \"\"\"\n",
    "    return cities[city_code]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataframe row count: 10000\n",
      "+-----+------+------+------+------+-------+-------+-------+-------+-------+------+-------+-----+--------+--------+-----+-------+-------+-------+-------+-------+--------+------+------+-------+-------------+-----+--------+\n",
      "|cicid| i94yr|i94mon|i94cit|i94res|i94port|arrdate|i94mode|i94addr|depdate|i94bir|i94visa|count|dtadfile|visapost|occup|entdepa|entdepd|entdepu|matflag|biryear| dtaddto|gender|insnum|airline|       admnum|fltno|visatype|\n",
      "+-----+------+------+------+------+-------+-------+-------+-------+-------+------+-------+-----+--------+--------+-----+-------+-------+-------+-------+-------+--------+------+------+-------+-------------+-----+--------+\n",
      "|  6.0|2016.0|   4.0| 692.0| 692.0|    XXX|20573.0|   null|   null|   null|  37.0|    2.0|  1.0|    null|    null| null|      T|   null|      U|   null| 1979.0|10282016|  null|  null|   null|1.897628485E9| null|      B2|\n",
      "|  7.0|2016.0|   4.0| 254.0| 276.0|    ATL|20551.0|    1.0|     AL|   null|  25.0|    3.0|  1.0|20130811|     SEO| null|      G|   null|      Y|   null| 1991.0|     D/S|     M|  null|   null| 3.73679633E9|00296|      F1|\n",
      "+-----+------+------+------+------+-------+-------+-------+-------+-------+------+-------+-----+--------+--------+-----+-------+-------+-------+-------+-------+--------+------+------+-------+-------------+-----+--------+\n",
      "only showing top 2 rows\n",
      "\n",
      "Dataframe first 2 rows: None\n",
      "root\n",
      " |-- cicid: double (nullable = true)\n",
      " |-- i94yr: double (nullable = true)\n",
      " |-- i94mon: double (nullable = true)\n",
      " |-- i94cit: double (nullable = true)\n",
      " |-- i94res: double (nullable = true)\n",
      " |-- i94port: string (nullable = true)\n",
      " |-- arrdate: double (nullable = true)\n",
      " |-- i94mode: double (nullable = true)\n",
      " |-- i94addr: string (nullable = true)\n",
      " |-- depdate: double (nullable = true)\n",
      " |-- i94bir: double (nullable = true)\n",
      " |-- i94visa: double (nullable = true)\n",
      " |-- count: double (nullable = true)\n",
      " |-- dtadfile: string (nullable = true)\n",
      " |-- visapost: string (nullable = true)\n",
      " |-- occup: string (nullable = true)\n",
      " |-- entdepa: string (nullable = true)\n",
      " |-- entdepd: string (nullable = true)\n",
      " |-- entdepu: string (nullable = true)\n",
      " |-- matflag: string (nullable = true)\n",
      " |-- biryear: double (nullable = true)\n",
      " |-- dtaddto: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- insnum: string (nullable = true)\n",
      " |-- airline: string (nullable = true)\n",
      " |-- admnum: double (nullable = true)\n",
      " |-- fltno: string (nullable = true)\n",
      " |-- visatype: string (nullable = true)\n",
      "\n",
      "Dataframe schema: None\n"
     ]
    }
   ],
   "source": [
    "debug_df(travel_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# The files *.codes are an extraction of valid codes from I94_SAS_Labels_Descriptions.SAS\n",
    "# This approach has be taken due to the short list of codes and to avoid relying on I94_SAS_Labels_Descriptions.SAS formats\n",
    "cit_res_codes = get_valid_codes(\"./i94codes/I94CIT-I94RES.codes\", isnumeric=True)\n",
    "port_codes = get_valid_codes(\"./i94codes/I94PORT.codes\", isnumeric=False)\n",
    "mode_codes = get_valid_codes(\"./i94codes/I94MODE.codes\", isnumeric=True)\n",
    "addr_codes = get_valid_codes(\"./i94codes/I94ADDR.codes\", isnumeric=False)\n",
    "\n",
    "travel_df = travel_df\\\n",
    "    .withColumn(\"i94port\", sanitize(travel_df.i94port))\\\n",
    "    .filter(travel_df.i94cit.isin(cit_res_codes))\\\n",
    "    .filter(travel_df.i94res.isin(cit_res_codes))\\\n",
    "    .filter(travel_df.i94port.isin(port_codes))\\\n",
    "    .filter(travel_df.i94mode.isin(mode_codes))\\\n",
    "    .filter(travel_df.i94addr.isin(addr_codes))\\\n",
    "    .where(travel_df.arrdate.isNotNull())\n",
    "\n",
    "travel_df = travel_df\\\n",
    "    .withColumn(\"city_name\", citycode_to_name(travel_df.i94port))\\\n",
    "    .withColumn(\"date\", sasdate_to_datetime(travel_df.arrdate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataframe row count: 9586\n",
      "+-----+------+------+------+------+-------+-------+-------+-------+-------+------+-------+-----+--------+--------+-----+-------+-------+-------+-------+-------+--------+------+------+-------+--------------+-----+--------+-------------+----------+\n",
      "|cicid| i94yr|i94mon|i94cit|i94res|i94port|arrdate|i94mode|i94addr|depdate|i94bir|i94visa|count|dtadfile|visapost|occup|entdepa|entdepd|entdepu|matflag|biryear| dtaddto|gender|insnum|airline|        admnum|fltno|visatype|    city_name|      date|\n",
      "+-----+------+------+------+------+-------+-------+-------+-------+-------+------+-------+-----+--------+--------+-----+-------+-------+-------+-------+-------+--------+------+------+-------+--------------+-----+--------+-------------+----------+\n",
      "| 15.0|2016.0|   4.0| 101.0| 101.0|    WAS|20545.0|    1.0|     MI|20691.0|  55.0|    2.0|  1.0|20160401|    null| null|      T|      O|   null|      M| 1961.0|09302016|     M|  null|     OS|  6.66643185E8|   93|      B2|WASHINGTON DC|2016-04-01|\n",
      "| 16.0|2016.0|   4.0| 101.0| 101.0|    NYC|20545.0|    1.0|     MA|20567.0|  28.0|    2.0|  1.0|20160401|    null| null|      O|      O|   null|      M| 1988.0|09302016|  null|  null|     AA|9.246846133E10|00199|      B2| NEW YORK, NY|2016-04-01|\n",
      "+-----+------+------+------+------+-------+-------+-------+-------+-------+------+-------+-----+--------+--------+-----+-------+-------+-------+-------+-------+--------+------+------+-------+--------------+-----+--------+-------------+----------+\n",
      "only showing top 2 rows\n",
      "\n",
      "Dataframe first 2 rows: None\n",
      "root\n",
      " |-- cicid: double (nullable = true)\n",
      " |-- i94yr: double (nullable = true)\n",
      " |-- i94mon: double (nullable = true)\n",
      " |-- i94cit: double (nullable = true)\n",
      " |-- i94res: double (nullable = true)\n",
      " |-- i94port: string (nullable = true)\n",
      " |-- arrdate: double (nullable = true)\n",
      " |-- i94mode: double (nullable = true)\n",
      " |-- i94addr: string (nullable = true)\n",
      " |-- depdate: double (nullable = true)\n",
      " |-- i94bir: double (nullable = true)\n",
      " |-- i94visa: double (nullable = true)\n",
      " |-- count: double (nullable = true)\n",
      " |-- dtadfile: string (nullable = true)\n",
      " |-- visapost: string (nullable = true)\n",
      " |-- occup: string (nullable = true)\n",
      " |-- entdepa: string (nullable = true)\n",
      " |-- entdepd: string (nullable = true)\n",
      " |-- entdepu: string (nullable = true)\n",
      " |-- matflag: string (nullable = true)\n",
      " |-- biryear: double (nullable = true)\n",
      " |-- dtaddto: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- insnum: string (nullable = true)\n",
      " |-- airline: string (nullable = true)\n",
      " |-- admnum: double (nullable = true)\n",
      " |-- fltno: string (nullable = true)\n",
      " |-- visatype: string (nullable = true)\n",
      " |-- city_name: string (nullable = true)\n",
      " |-- date: string (nullable = true)\n",
      "\n",
      "Dataframe schema: None\n"
     ]
    }
   ],
   "source": [
    "debug_df(travel_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "##### Temperatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataframe row count: 10000\n",
      "+----------+------------------+-----------------------------+-----+-------+--------+---------+\n",
      "|        dt|AverageTemperature|AverageTemperatureUncertainty| City|Country|Latitude|Longitude|\n",
      "+----------+------------------+-----------------------------+-----+-------+--------+---------+\n",
      "|1743-11-01|             6.068|           1.7369999999999999|Århus|Denmark|  57.05N|   10.33E|\n",
      "|1743-12-01|              null|                         null|Århus|Denmark|  57.05N|   10.33E|\n",
      "+----------+------------------+-----------------------------+-----+-------+--------+---------+\n",
      "only showing top 2 rows\n",
      "\n",
      "Dataframe first 2 rows: None\n",
      "root\n",
      " |-- dt: string (nullable = true)\n",
      " |-- AverageTemperature: string (nullable = true)\n",
      " |-- AverageTemperatureUncertainty: string (nullable = true)\n",
      " |-- City: string (nullable = true)\n",
      " |-- Country: string (nullable = true)\n",
      " |-- Latitude: string (nullable = true)\n",
      " |-- Longitude: string (nullable = true)\n",
      "\n",
      "Dataframe schema: None\n"
     ]
    }
   ],
   "source": [
    "debug_df(temp_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "temp_df = temp_df\\\n",
    "    .dropDuplicates(['dt', 'Latitude', 'Longitude'])\\\n",
    "    .dropDuplicates(['dt', 'City', 'Country'])\\\n",
    "    .where(temp_df.AverageTemperature.isNotNull())\\\n",
    "    .withColumn(\"City\", sanitize(temp_df['City']))\\\n",
    "    .withColumn(\"Country\", sanitize(temp_df['Country']))\\\n",
    "    .withColumn(\"month\", month(temp_df['dt']))\\\n",
    "    .withColumn(\"year\", year(temp_df['dt']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataframe row count: 9848\n",
      "+----------+------------------+-----------------------------+-----+-------+--------+---------+-----+----+\n",
      "|        dt|AverageTemperature|AverageTemperatureUncertainty| City|Country|Latitude|Longitude|month|year|\n",
      "+----------+------------------+-----------------------------+-----+-------+--------+---------+-----+----+\n",
      "|1743-11-01|             6.068|           1.7369999999999999|ÅRHUS|DENMARK|  57.05N|   10.33E|   11|1743|\n",
      "|1743-11-01|            10.013|                        2.291|ÇORLU| TURKEY|  40.99N|   27.69E|   11|1743|\n",
      "+----------+------------------+-----------------------------+-----+-------+--------+---------+-----+----+\n",
      "only showing top 2 rows\n",
      "\n",
      "Dataframe first 2 rows: None\n",
      "root\n",
      " |-- dt: string (nullable = true)\n",
      " |-- AverageTemperature: string (nullable = true)\n",
      " |-- AverageTemperatureUncertainty: string (nullable = true)\n",
      " |-- City: string (nullable = true)\n",
      " |-- Country: string (nullable = true)\n",
      " |-- Latitude: string (nullable = true)\n",
      " |-- Longitude: string (nullable = true)\n",
      " |-- month: integer (nullable = true)\n",
      " |-- year: integer (nullable = true)\n",
      "\n",
      "Dataframe schema: None\n"
     ]
    }
   ],
   "source": [
    "debug_df(temp_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "##### Demographics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataframe row count: 2891\n",
      "+-------------+-------------+----------+---------------+-----------------+----------------+------------------+------------+----------------------+----------+------------------+-----+\n",
      "|         City|        State|Median Age|Male Population|Female Population|Total Population|Number of Veterans|Foreign-born|Average Household Size|State Code|              Race|Count|\n",
      "+-------------+-------------+----------+---------------+-----------------+----------------+------------------+------------+----------------------+----------+------------------+-----+\n",
      "|Silver Spring|     Maryland|      33.8|          40601|            41862|           82463|              1562|       30908|                   2.6|        MD|Hispanic or Latino|25924|\n",
      "|       Quincy|Massachusetts|      41.0|          44129|            49500|           93629|              4147|       32935|                  2.39|        MA|             White|58723|\n",
      "+-------------+-------------+----------+---------------+-----------------+----------------+------------------+------------+----------------------+----------+------------------+-----+\n",
      "only showing top 2 rows\n",
      "\n",
      "Dataframe first 2 rows: None\n",
      "root\n",
      " |-- City: string (nullable = true)\n",
      " |-- State: string (nullable = true)\n",
      " |-- Median Age: string (nullable = true)\n",
      " |-- Male Population: string (nullable = true)\n",
      " |-- Female Population: string (nullable = true)\n",
      " |-- Total Population: string (nullable = true)\n",
      " |-- Number of Veterans: string (nullable = true)\n",
      " |-- Foreign-born: string (nullable = true)\n",
      " |-- Average Household Size: string (nullable = true)\n",
      " |-- State Code: string (nullable = true)\n",
      " |-- Race: string (nullable = true)\n",
      " |-- Count: string (nullable = true)\n",
      "\n",
      "Dataframe schema: None\n"
     ]
    }
   ],
   "source": [
    "debug_df(demog_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "demog_df = demog_df\\\n",
    "    .dropDuplicates(['City', 'State Code'])\\\n",
    "    .withColumn('City', sanitize(demog_df.City))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataframe row count: 596\n",
      "+-------+-----+----------+---------------+-----------------+----------------+------------------+------------+----------------------+----------+--------------------+-----+\n",
      "|   City|State|Median Age|Male Population|Female Population|Total Population|Number of Veterans|Foreign-born|Average Household Size|State Code|                Race|Count|\n",
      "+-------+-----+----------+---------------+-----------------+----------------+------------------+------------+----------------------+----------+--------------------+-----+\n",
      "|ABILENE|Texas|      31.3|          65212|            60664|          125876|              9367|        8129|                  2.64|        TX|American Indian a...| 1813|\n",
      "|  AKRON| Ohio|      38.1|          96886|           100667|          197553|             12878|       10024|                  2.24|        OH|Black or African-...|66551|\n",
      "+-------+-----+----------+---------------+-----------------+----------------+------------------+------------+----------------------+----------+--------------------+-----+\n",
      "only showing top 2 rows\n",
      "\n",
      "Dataframe first 2 rows: None\n",
      "root\n",
      " |-- City: string (nullable = true)\n",
      " |-- State: string (nullable = true)\n",
      " |-- Median Age: string (nullable = true)\n",
      " |-- Male Population: string (nullable = true)\n",
      " |-- Female Population: string (nullable = true)\n",
      " |-- Total Population: string (nullable = true)\n",
      " |-- Number of Veterans: string (nullable = true)\n",
      " |-- Foreign-born: string (nullable = true)\n",
      " |-- Average Household Size: string (nullable = true)\n",
      " |-- State Code: string (nullable = true)\n",
      " |-- Race: string (nullable = true)\n",
      " |-- Count: string (nullable = true)\n",
      "\n",
      "Dataframe schema: None\n"
     ]
    }
   ],
   "source": [
    "debug_df(demog_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 3: Define the Data Model\n",
    "#### 3.1 Conceptual Data Model\n",
    "\n",
    "The model will used a star schema. Aggregating or partioning is almost already possible on cities across the staging frames / tables previously created. Time is also a good candidate for as a dimension and to look at trends over time (ie. temperature vs. travel dates).\n",
    "\n",
    "The reasoning behind the use of a star schema vs a snowflake schema. Both are commonly used models for data-warehousing but the star model:\n",
    "* decreases the chance of data integrity problems and is simpler to maintain\n",
    "* reduce query complexity and query response time (less JOINs)\n",
    "\n",
    "We will not include any information about traveler origin to avoid any sort of bias on origins or genders.\n",
    "\n",
    "The start schema includes the following tables:\n",
    "\n",
    "* `location_dim` table: `city_name`, `state`, `country_name`, `longitude`, `latitude`\n",
    "* `time_dim`: `date`, `day`, `weekday`, `week`, `month`, `year`\n",
    "* `temperature_dim: `city_code`, `avg_temperature`, `month`, `year`\n",
    "* `traveler_fact`: `id`, `arrival_date`, `departure_date`, `visa_type`, `month`, `year`\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 3.2 Mapping Out Data Pipelines\n",
    "\n",
    "The data pipeline will have the following steps listed in order:\n",
    "\n",
    "1. Clean data\n",
    "2. Store data into staging table (schema-on read with Spark)\n",
    "3. Transform 3FN staging table into a star schema with Spark SQL\n",
    "4. Add a simple data quality that guarantee the presence of rows into the target tables after transformation\n",
    "5. Partition data per city, month, year\n",
    "6. Save data tables into Parquet files that can be re-used for a future analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 4: Run Pipelines to Model the Data \n",
    "#### 4.1 Create the data model\n",
    "Build the data pipelines to create the data model with of use of temporary views cretfrom Spark dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "temp_staging = \"temp_staging\"\n",
    "temp_df.createOrReplaceTempView(temp_staging)\n",
    "\n",
    "demog_staging = \"demog_staging\"\n",
    "demog_df.createOrReplaceTempView(demog_staging)\n",
    "\n",
    "travel_staging = \"travel_staging\"\n",
    "travel_df.createOrReplaceTempView(travel_staging)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- dt: string (nullable = true)\n",
      " |-- AverageTemperature: string (nullable = true)\n",
      " |-- AverageTemperatureUncertainty: string (nullable = true)\n",
      " |-- City: string (nullable = true)\n",
      " |-- Country: string (nullable = true)\n",
      " |-- Latitude: string (nullable = true)\n",
      " |-- Longitude: string (nullable = true)\n",
      " |-- month: integer (nullable = true)\n",
      " |-- year: integer (nullable = true)\n",
      "\n",
      "root\n",
      " |-- City: string (nullable = true)\n",
      " |-- State: string (nullable = true)\n",
      " |-- Median Age: string (nullable = true)\n",
      " |-- Male Population: string (nullable = true)\n",
      " |-- Female Population: string (nullable = true)\n",
      " |-- Total Population: string (nullable = true)\n",
      " |-- Number of Veterans: string (nullable = true)\n",
      " |-- Foreign-born: string (nullable = true)\n",
      " |-- Average Household Size: string (nullable = true)\n",
      " |-- State Code: string (nullable = true)\n",
      " |-- Race: string (nullable = true)\n",
      " |-- Count: string (nullable = true)\n",
      "\n",
      "root\n",
      " |-- cicid: double (nullable = true)\n",
      " |-- i94yr: double (nullable = true)\n",
      " |-- i94mon: double (nullable = true)\n",
      " |-- i94cit: double (nullable = true)\n",
      " |-- i94res: double (nullable = true)\n",
      " |-- i94port: string (nullable = true)\n",
      " |-- arrdate: double (nullable = true)\n",
      " |-- i94mode: double (nullable = true)\n",
      " |-- i94addr: string (nullable = true)\n",
      " |-- depdate: double (nullable = true)\n",
      " |-- i94bir: double (nullable = true)\n",
      " |-- i94visa: double (nullable = true)\n",
      " |-- count: double (nullable = true)\n",
      " |-- dtadfile: string (nullable = true)\n",
      " |-- visapost: string (nullable = true)\n",
      " |-- occup: string (nullable = true)\n",
      " |-- entdepa: string (nullable = true)\n",
      " |-- entdepd: string (nullable = true)\n",
      " |-- entdepu: string (nullable = true)\n",
      " |-- matflag: string (nullable = true)\n",
      " |-- biryear: double (nullable = true)\n",
      " |-- dtaddto: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- insnum: string (nullable = true)\n",
      " |-- airline: string (nullable = true)\n",
      " |-- admnum: double (nullable = true)\n",
      " |-- fltno: string (nullable = true)\n",
      " |-- visatype: string (nullable = true)\n",
      " |-- city_name: string (nullable = true)\n",
      " |-- date: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "temp_df.printSchema()\n",
    "demog_df.printSchema()\n",
    "travel_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT DISTINCT\n",
    "    cicid as id, \n",
    "    visatype as visa_type, \n",
    "    i94mon as month, \n",
    "    i94yr as year, \n",
    "    i94cit as city_name\n",
    "FROM {}\n",
    "\"\"\"\\\n",
    "    .format(travel_staging))\\\n",
    "    .write\\\n",
    "    .partitionBy(\"month\", \"year\")\\\n",
    "    .parquet(\"dim_fact/traveler_fact.parquet\", mode=\"Overwrite\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT DISTINCT\n",
    "   city as city_name,\n",
    "   AverageTemperature as avg_temperature, \n",
    "   month, \n",
    "   year\n",
    "FROM {}\n",
    "\"\"\"\\\n",
    "    .format(temp_staging))\\\n",
    "    .write\\\n",
    "    .partitionBy(\"month\", \"year\")\\\n",
    "    .parquet(\"dim_fact/temperature_dim.parquet\", mode=\"Overwrite\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT DISTINCT\n",
    "    demographics.city as city_name,\n",
    "    country as country_name,\n",
    "    state as state_name,\n",
    "    longitude, \n",
    "    latitude\n",
    "FROM {} demographics\n",
    "JOIN temp_staging temperatures ON\n",
    "    demographics.city = temperatures.city\n",
    "\"\"\"\\\n",
    "    .format(demog_staging, temp_staging))\\\n",
    "    .write\\\n",
    "    .partitionBy(\"state_name\", \"country_name\")\\\n",
    "    .parquet(\"dim_fact/location_dim.parquet\", mode=\"Overwrite\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "spark.sql(\"\"\"\n",
    "SELECT DISTINCT\n",
    "    date,\n",
    "    dayofmonth(date) as day,\n",
    "    dayofweek(date) as weekday,\n",
    "    weekofyear(date) as week,\n",
    "    month(date) as month,\n",
    "    year(date) as year\n",
    "FROM {}\n",
    "\"\"\"\\\n",
    "    .format(travel_staging))\\\n",
    "    .write\\\n",
    "    .partitionBy(\"month\", \"year\")\\\n",
    "    .parquet(\"dim_fact/time_dim.parquet\", mode=\"Overwrite\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 4.2 Data Quality Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test passed: 1 records found for dim_fact/time_dim.parquet\n",
      "Test passed: 9848 records found for dim_fact/temperature_dim.parquet\n",
      "Test passed: 9586 records found for dim_fact/traveler_fact.parquet\n"
     ]
    }
   ],
   "source": [
    "def it_should_have_records(parquet_file):\n",
    "    \"\"\"\n",
    "    Verify that the number of row in the table is greater than 0\n",
    "    \"\"\"\n",
    "    parquetData = spark.read.parquet(parquet_file)\n",
    "    if (parquetData.count() <= 0):\n",
    "        raise ValueError(\"The number of records in {} must greater than 0\".format(parquet_file))\n",
    "    else:\n",
    "        print(\"Test passed: {} records found for {}\".format(parquetData.count(), parquet_file))\n",
    "\n",
    "it_should_have_records(\"dim_fact/time_dim.parquet\")\n",
    "it_should_have_records(\"dim_fact/temperature_dim.parquet\")\n",
    "it_should_have_records(\"dim_fact/traveler_fact.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 4.3 Data dictionary \n",
    "\n",
    "Brief description of what the data is and where it came from:\n",
    "\n",
    "`location_dim` table: \n",
    "* `city_name`: City name\n",
    "* `state`: State\n",
    "* `country_name`: Country name\n",
    "* `longitude`: geo-localisation info.\n",
    "* `latitude`: geo-localisation info.\n",
    "\n",
    "`time_dim` table: \n",
    "* `date`: ISO date\n",
    "* `day`: day\n",
    "* `weekday`: day of the week\n",
    "* `week`: week of the year\n",
    "* `month`: month\n",
    "* `year`: year\n",
    "\n",
    "`temperature_dim` table: \n",
    "* `city_name`: City name\n",
    "* `avg_temperature`: average temperature for the month\n",
    "* `month`: month of the year\n",
    "* `year`: year\n",
    "\n",
    "`traveler_fact` table: \n",
    "* `id`: traveler unique id\n",
    "* `arrival_date`: traveler arrival date\n",
    "* `visa_type`: traveler visa type (ie. B2)\n",
    "* `month`: visa delivery month\n",
    "* `year`: visa delivery year\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Step 5: Complete Project Write Up\n",
    "* Rationale for the choice of tools and technologies for the project:\n",
    "\n",
    "Spark was chosen to enable MPP (Mass Parallel Processing) due to the size of the dataset.\n",
    "\n",
    "Spark SQL simplifies reading values from different files formats (SAS, CSV, JSON) and provides schema-on read which reduces the time spent in modelling tables.\n",
    "\n",
    "UDF functions were also used for Spark to process data for existing or new columns that needed particular processing (ie. date conversions).\n",
    "\n",
    "The data should be updated on regular basis. The current project does not integrate with AWS S3 but this change would be pretty seamless and guarantee that the pipeline can get data from different sources that get updated by upstream producers. The importance of frequently updating data depends on when consumers will need it. Depending on the frequency, the pipeline could use a time-based approach in Apache Airflow to split the data in small tasks within DAGs that can then be executed quickly.\n",
    "\n",
    "Changes in frequency could affect the resources (memory, disk) of AWS EMR or Airflow workers, so understanding these requirements will need to continuously observed.\n",
    "\n",
    " * If the data was increased by 100x, it is recommended to monitor/re-calibrate the current resources (memory, disk, CPU) to accomodate the increased capacity, whether it's Spark Nodes or Airflow workers. CPU will likely be the metrics the most impacted and data distribution (related to partioning and replication) will need to analysed with a likely increased number of new use cases. It is important to measure and observe these changes because relying on Cloud managed services does not solve all these challenges if configurations and data processing performance is not aligned with usages. Ie. [Optimize Amazon S3 for High Concurrency in Distributed Workloads](https://aws.amazon.com/blogs/big-data/optimizing-amazon-s3-for-high-concurrency-in-distributed-workloads/)\n",
    " \n",
    " * If the data populates a dashboard that must be updated on a daily basis by 7am every day, Airflow schedules and automated alerts (ie. Slack) would be very helpful and help to quickly narrow down errors/mistakes preventing data processing.\n",
    " \n",
    " * If the database needed to be accessed by 100+ people, a data-warehouse like AWS RedShift or GCP BigQuery should be considered as these systems are designed to optimise data storage and read access for a large number of concurrent users."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
